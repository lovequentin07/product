# 완료

# 데이터 로컬라이징 및 하이브리드 업데이트 계획

## 배경 및 목적
- 공공데이터 API의 파라미터 제약(아파트명, 연월 필수) 해결
- 조회 성능 향상 및 자유로운 데이터 필터링(가격순, 면적순 등) 구현
- API 호출 횟수 최적화

## 1단계: 준비 및 브랜치 전략
- [x] 작업 브랜치 생성 (`feat/data-infrastructure`)
- [x] 데이터 수집용 정식 디렉토리 설정 (`raw-data/`) 및 `.gitignore` 확인

## 2단계: 데이터 벌크 다운로드 및 구조화
- [x] **파라미터 조합 생성**: `generate-combinations.ts`로 2006년~2026년 전체 조합 확보
- [x] **모든 컬럼 수집**: `fetch-historical.ts` 스크립트가 API 응답의 모든 필드를 수집하도록 수정
- [x] **수집 동시 구조화**: 별도 분할 과정 없이, 수집 시 `raw-data/seoul/YYYY/MM.jsonl` 경로에 즉시 분할 저장하도록 `fetch-historical.ts` 로직 개선 완료. (`split-data.ts` 불필요)
- [ ] **전체 데이터 수집 실행**: API 제한을 피해 전체 기간 데이터 수집 완료 (현재 약 61% 진행)

---

## 🚀 남은 주요 작업

### 3단계: 프로덕션 저장소 구축 (Cloudflare R2 & D1)
- [x] **저장소 최종 결정**:
    - **Cloudflare R2**: 원본 데이터(`.jsonl`) 백업 및 데이터 레이크 역할로 사용 결정.
    - **Cloudflare D1**: 실제 서비스의 빠른 조회를 위한 주 데이터베이스(SQLite)로 사용 결정.
- [ ] **D1 데이터베이스 스키마 설계**: `transactions` 테이블을 생성하고, 검색에 필요한 컬럼(가격, 지역, 날짜 등)에 인덱스(Index)를 설정하는 스키마 설계.
- [ ] **R2 데이터 업로드**: 로컬에 수집 완료된 `raw-data/seoul/`의 모든 `.jsonl` 파일들을 R2 버킷에 업로드하는 스크립트 작성 및 실행.
- [ ] **D1 데이터 마이그레이션**: R2에 업로드된 `.jsonl` 파일을 읽어 D1 데이터베이스 `transactions` 테이블에 삽입(insert)하는 스크립트 작성 및 실행.

### 4단계: API 레이어 교체 및 검증
- [ ] **내부 데이터 조회 API 개발**: 기존 공공 API 호출 로직을 **D1 데이터베이스를 조회**하는 새로운 내부 API(예: `/api/transactions`)로 전환.
- [ ] **성능 최적화**: D1 쿼리 최적화 및 Cloudflare 캐싱 전략을 적용하여 API 응답 속도 극대화.
- [ ] **프론트엔드 연동**: `/real-estate/transaction` 페이지가 새로운 내부 API를 호출하도록 수정.
- [ ] **기능 테스트**: 정렬, 필터링 등 모든 기능이 D1 기반으로 정상 작동하는지 통합 테스트.

### 5단계: 증분 업데이트(Daily Sync) 자동화
- [ ] **업데이트 스크립트 작성**: 매일 최신 월 데이터를 API에서 가져와 R2와 D1을 모두 업데이트하는 스크립트 구현.
- [ ] **자동화 스케줄링**: Cloudflare Cron Trigger를 이용해 매일 새벽 업데이트 스크립트가 자동으로 실행되도록 설정.
- [ ] **배포**: 전체 스택을 Cloudflare 환경에 배포.
